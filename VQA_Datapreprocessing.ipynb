{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "88a001b2",
   "metadata": {},
   "source": [
    "   # Neuroscience inspired AI\n",
    "   \n",
    "   \n",
    "   ## Implementing LSTMs using TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4840975",
   "metadata": {},
   "source": [
    "## Objective - \n",
    "\n",
    "\n",
    "To develop hypotheses and working concepts to achieve multimodal inference for scenarios where questions are input via text to images or videos that the model can adequately answer.\n",
    "The hypothesis will be based on Visual Question Answering (VQA)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5885164f",
   "metadata": {},
   "source": [
    "##### Visual Question Answering (VQA) is a task that combines computer vision, natural language processing, and deep learning.\n",
    " \n",
    "##### VQA is the phenomenon of freely asking questions in natural language about visual (image/video) content.However, answering these questions requires a wide range of skills. These skills include proper localization and recognition of objects, people, their activities, and common sense.\n",
    "\n",
    "## Task - \n",
    "\n",
    "\n",
    "Given an image, a visual question-answering algorithm allows the machine to answer free-form, Open-ended, natural-language questions about the image.\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4df13ff3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://miro.medium.com/max/1400/1*yqBRejJjQeQ55DjHVpTqfA.png\" width=\"500\" height=\"500\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#import image module\n",
    "from IPython.display import Image\n",
    "\n",
    "# get the image\n",
    "Image(url=\"https://miro.medium.com/max/1400/1*yqBRejJjQeQ55DjHVpTqfA.png\", width=500, height=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "216b615e",
   "metadata": {},
   "source": [
    "## Experimental Evalution :\n",
    "\n",
    "(1) Study and model the bias in the Visual7W, COCO and COCO-QA Multiple Choice datasets, \n",
    "\n",
    "(2) measure the effect of using visual features from different CNN architectures, \n",
    "\n",
    "(3) explore the use of a LSTM as the systemâ€™s language model, and \n",
    "\n",
    "(4) study transferability of our model between datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "93767a3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.9.1\n"
     ]
    }
   ],
   "source": [
    "#importing necessary libraries and frameworks\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee9a0561",
   "metadata": {},
   "source": [
    "## Dataset : Visual Question Answering\n",
    "\n",
    "#### Visual7W\n",
    "\n",
    "Visual7W is a large-scale visual question answering (QA) dataset, with object-level groundings and multimodal answers. Each question starts with one of the seven Ws, what, where, when, who, why, how and which. It is collected from 47,300 COCO iamges and it has 327,929 QA pairs, together with 1,311,756 human-generated multiple-choices and 561,459 object groundings from 36,579 categories.\n",
    "\n",
    "#### COCO (Microsoft Common Objects in Context)\n",
    "\n",
    "The MS COCO (Microsoft Common Objects in Context) dataset is a large-scale object detection, segmentation, key-point detection, and captioning dataset. The dataset consists of 328K images.\n",
    "\n",
    "Splits: The first version of MS COCO dataset was released in 2014. It contains 164K images split into training (83K), validation (41K) and test (41K) sets. In 2015 additional test set of 81K images was released, including all the previous test images and 40K new images.\n",
    "\n",
    "#### COCO-QA\n",
    "\n",
    "COCO-QA is a dataset for visual question answering. It consists of:\n",
    "\n",
    "123287 images\n",
    "\n",
    "78736 train questions\n",
    "\n",
    "38948 test questions\n",
    "\n",
    "4 types of questions: object, number, color, location\n",
    "\n",
    "Answers are all one-word."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a592655",
   "metadata": {},
   "source": [
    "## Loading Datasets :"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "661fa6b3",
   "metadata": {},
   "source": [
    "#### Creating Account in Hub"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ca0d18d",
   "metadata": {},
   "source": [
    "## Model Experiments on 7W QA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cd077b1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: wget in /opt/anaconda3/lib/python3.9/site-packages (3.2)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install wget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "da14d1f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import argparse\n",
    "import wget\n",
    "\n",
    "def download_vqa():\n",
    "    os.system('wget https://s3.amazonaws.com/cvmlp/vqa/mscoco/vqa/v2_Questions_Train_mscoco.zip -P zip/')\n",
    "    os.system('wget https://s3.amazonaws.com/cvmlp/vqa/mscoco/vqa/v2_Questions_Val_mscoco.zip -P zip/')\n",
    "    os.system('wget https://s3.amazonaws.com/cvmlp/vqa/mscoco/vqa/v2_Questions_Test_mscoco.zip -P zip/')\n",
    "\n",
    "    # Download the VQA Annotations\n",
    "    os.system('wget https://s3.amazonaws.com/cvmlp/vqa/mscoco/vqa/v2_Annotations_Train_mscoco.zip -P zip/')\n",
    "    os.system('wget https://s3.amazonaws.com/cvmlp/vqa/mscoco/vqa/v2_Annotations_Val_mscoco.zip -P zip/')\n",
    "\n",
    "\n",
    "    # Unzip the annotations\n",
    "    os.system('unzip zip/v2_Questions_Train_mscoco.zip -d annotations/')\n",
    "    os.system('unzip zip/v2_Questions_Val_mscoco.zip -d annotations/')\n",
    "    os.system('unzip zip/v2_Questions_Test_mscoco.zip -d annotations/')\n",
    "    os.system('unzip zip/v2_Annotations_Train_mscoco.zip -d annotations/')\n",
    "    os.system('unzip zip/v2_Annotations_Val_mscoco.zip -d annotations/')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7a5df104",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sh: wget: command not found\n",
      "sh: wget: command not found\n",
      "sh: wget: command not found\n",
      "sh: wget: command not found\n",
      "sh: wget: command not found\n",
      "unzip:  cannot find or open zip/v2_Questions_Train_mscoco.zip, zip/v2_Questions_Train_mscoco.zip.zip or zip/v2_Questions_Train_mscoco.zip.ZIP.\n",
      "unzip:  cannot find or open zip/v2_Questions_Val_mscoco.zip, zip/v2_Questions_Val_mscoco.zip.zip or zip/v2_Questions_Val_mscoco.zip.ZIP.\n",
      "unzip:  cannot find or open zip/v2_Questions_Test_mscoco.zip, zip/v2_Questions_Test_mscoco.zip.zip or zip/v2_Questions_Test_mscoco.zip.ZIP.\n",
      "unzip:  cannot find or open zip/v2_Annotations_Train_mscoco.zip, zip/v2_Annotations_Train_mscoco.zip.zip or zip/v2_Annotations_Train_mscoco.zip.ZIP.\n",
      "unzip:  cannot find or open zip/v2_Annotations_Val_mscoco.zip, zip/v2_Annotations_Val_mscoco.zip.zip or zip/v2_Annotations_Val_mscoco.zip.ZIP.\n"
     ]
    }
   ],
   "source": [
    "download_vqa()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b3d1a8d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63c394b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f10a1d8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27d4c182",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d16f74db",
   "metadata": {},
   "source": [
    "# PK - Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4cf2434d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: wget in /opt/anaconda3/lib/python3.9/site-packages (3.2)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install wget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09b8e8e9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "48822f6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100% [....................................................] 10518930 / 10518930"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'v2_Annotations_Val_mscoco (2).zip'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wget\n",
    "\n",
    "Training_coco = 'https://s3.amazonaws.com/cvmlp/vqa/mscoco/vqa/v2_Questions_Train_mscoco.zip'\n",
    "Validation_coco = 'https://s3.amazonaws.com/cvmlp/vqa/mscoco/vqa/v2_Questions_Val_mscoco.zip'\n",
    "Training_ann = 'https://s3.amazonaws.com/cvmlp/vqa/mscoco/vqa/v2_Annotations_Train_mscoco.zip'\n",
    "Validation_ann = 'https://s3.amazonaws.com/cvmlp/vqa/mscoco/vqa/v2_Annotations_Val_mscoco.zip'\n",
    "\n",
    "wget.download(Training_coco)\n",
    "wget.download(Validation_coco)\n",
    "wget.download(Training_ann)\n",
    "wget.download(Validation_ann)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "32d101f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import os, argparse\n",
    "import cv2, spacy, numpy as np\n",
    "from keras.models import model_from_json\n",
    "from keras.optimizers import SGD\n",
    "import joblib\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "84a6e049",
   "metadata": {},
   "outputs": [],
   "source": [
    "VQA_model_file_name      = 'models/VQA/VQA_MODEL.json'\n",
    "VQA_weights_file_name   = 'models/VQA/VQA_MODEL_WEIGHTS.hdf5'\n",
    "label_encoder_file_name  = 'models/VQA/FULL_labelencoder_trainval.pkl'\n",
    "CNN_weights_file_name   = 'models/CNN/vgg16_weights.h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f498c00c",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_file_name = 'elephant.jpg'\n",
    "question = u'What animal is in the picture?'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "995c7058",
   "metadata": {},
   "outputs": [],
   "source": [
    "verbose = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1e52587c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.lang.en import English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fb577430",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image_model(CNN_weights_file_name):\n",
    "    ''' Takes the CNN weights file, and returns the VGG model update \n",
    "    with the weights. Requires the file VGG.py inside models/CNN '''\n",
    "    from models.CNN.VGG import VGG_16\n",
    "    image_model = VGG_16(CNN_weights_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "910efa27",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/keras/optimizers/optimizer_v2/gradient_descent.py:108: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(SGD, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'image_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/b7/vzbyqvhj68x_f3mjxg3rn3g80000gn/T/ipykernel_7135/255248301.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# this is standard VGG 16 without the last two layers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0msgd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSGD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecay\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-6\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmomentum\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.9\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnesterov\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mimage_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msgd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'categorical_crossentropy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mreturn\u001b[0m \u001b[0mimage_model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'image_model' is not defined"
     ]
    }
   ],
   "source": [
    " # this is standard VGG 16 without the last two layers\n",
    "sgd = SGD(lr=0.1, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "image_model.compile(optimizer=sgd, loss='categorical_crossentropy')\n",
    "return image_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6fd215b",
   "metadata": {},
   "source": [
    "## Conclusion  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39fb6d12",
   "metadata": {},
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cde68cc",
   "metadata": {},
   "source": [
    "https://www.geeksforgeeks.org/insert-image-in-a-jupyter-notebook/#:~:text=first%2C%20change%20the%20type%20of,Edit%20%2D%3E%20insert%20image.\n",
    "\n",
    "\n",
    "https://paperswithcode.com/dataset/visual7w\n",
    "\n",
    "https://arxiv.org/pdf/1511.03416v4.pdf\n",
    "\n",
    "\n",
    "https://paperswithcode.com/dataset/coco\n",
    "\n",
    "\n",
    "https://paperswithcode.com/dataset/coco-qa\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0e547b7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
